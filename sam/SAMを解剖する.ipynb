{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMを解剖する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from cdt.utils.torch import ChannelBatchNorm1d, MatrixSampler, Linear3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "np.set_printoptions(precision=2, floatmode='fixed', suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_data=2000):\n",
    "\n",
    "    x = np.random.uniform(low=-1, high=1, size=n_data)  # -1から1の一様乱数\n",
    "\n",
    "    e_z = np.random.randn(n_data)  # ノイズの生成\n",
    "    z_prob = scipy.special.expit(-5.0 * x + 5 * e_z)\n",
    "    Z = np.array([])\n",
    "\n",
    "    for i in range(n_data):\n",
    "        Z_i = np.random.choice(2, size=1, p=[1-z_prob[i], z_prob[i]])[0]\n",
    "        Z = np.append(Z, Z_i)\n",
    "\n",
    "    t = np.zeros(n_data)\n",
    "    for i in range(n_data):\n",
    "        if x[i] < 0:\n",
    "            t[i] = 0.5\n",
    "        elif x[i] >= 0 and x[i] < 0.5:\n",
    "            t[i] = 0.7\n",
    "        elif x[i] >= 0.5:\n",
    "            t[i] = 1.0\n",
    "\n",
    "    e_y = np.random.randn(n_data)\n",
    "    Y = 2.0 + t*Z + 0.3*x + 0.1*e_y \n",
    "\n",
    "    Y2 = np.random.choice(\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "        n_data, p=[0.1, 0.2, 0.3, 0.2, 0.2]\n",
    "    )\n",
    "\n",
    "    e_y3 = np.random.randn(n_data)\n",
    "    Y3 = 3 * Y + Y2 + e_y3\n",
    "\n",
    "    e_y4 = np.random.randn(n_data)\n",
    "    Y4 = 3 * Y3 + 2 * e_y4 + 5\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'x': x,\n",
    "        'Z': Z,\n",
    "        'Y': Y,\n",
    "        'Y2': Y2,\n",
    "        'Y3': Y3,\n",
    "        'Y4': Y4\n",
    "    })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_data_col, n_hidden_layer, n_hidden_layers):\n",
    "        \n",
    "        super(SAMDiscriminator, self).__init__()\n",
    "        \n",
    "        self.n_data_col = n_data_col\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(n_data_col, n_hidden_layer))\n",
    "        layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "        layers.append(nn.LeakyReLU(.2))\n",
    "        \n",
    "        for i in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(n_hidden_layer, n_hidden_layer))\n",
    "            layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "            layers.append(nn.LeakyReLU(.2))\n",
    "            \n",
    "        layers.append(nn.Linear(n_hidden_layer, 1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'mask', torch.eye(n_data_col, n_data_col).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, obs_data=None):\n",
    "        \n",
    "        if obs_data is not None:\n",
    "            return [\n",
    "                self.layers(i) for i in torch.unbind(\n",
    "                    obs_data.unsqueeze(1) * (1 - self.mask) + input.unsqueeze(1) * self.mask,\n",
    "                    1\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            return self.layers(input)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_data_col, n_hidden_layer):\n",
    "        \n",
    "        super(SAMGenerator, self).__init__()\n",
    "        \n",
    "        skeleton = 1 - torch.eye(n_data_col + 1, n_data_col)\n",
    "        \n",
    "        self.register_buffer('skeleton', skeleton)\n",
    "        \n",
    "        self.input_layer = Linear3D((n_data_col, n_data_col + 1, n_hidden_layer))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(ChannelBatchNorm1d(n_data_col, n_hidden_layer))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.output_layer = Linear3D((n_data_col, n_hidden_layer, 1))\n",
    "        \n",
    "    def forward(self, data, noise, adj_matrix, drawn_neurons=None):\n",
    "        \n",
    "        x = self.input_layer(data, noise, adj_matrix * self.skeleton)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        output = self.output_layer(x, noise=None, adj_matrix=drawn_neurons)\n",
    "        \n",
    "        return output.squeeze(2)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        self.input_layer.reset_parameters()\n",
    "        self.output_layer.reset_parameters()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parametres'):\n",
    "                layer.register_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notears_constr(adj_m, max_pow=None):\n",
    "    \n",
    "    m_exp = [adj_m]\n",
    "    if max_pow is None:\n",
    "        max_pow = adj_m.shape[1]\n",
    "    while (m_exp[-1].sum() > 0 and len(m_exp) < max_pow):\n",
    "        m_exp.append(m_exp[-1] @ adj_m / len(m_exp))\n",
    "        \n",
    "    return sum([i.diag().sum() for idx, i in enumerate(m_exp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam_model(\n",
    "    data,\n",
    "    n_hidden_layer_gen=100, n_hidden_layer_dis=100,\n",
    "    n_hidden_layers_dis=2,\n",
    "    lr_gen=0.01*0.5, lr_dis=0.01*0.5*2,\n",
    "    dag_start_rate=0.5, dag_penalization_increase=0.001*10,\n",
    "    epochs_train=100, epochs_test=100,\n",
    "    lambda1=5.0*20, lambda2=0.005*20\n",
    "):\n",
    "\n",
    "    data_columns = data.columns.tolist() \n",
    "    n_data_col = len(data_columns)  \n",
    "    data = torch.from_numpy(data.values.astype('float32') )\n",
    "    batch_size = len(data)\n",
    "\n",
    "    data_iterator = DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    sam = SAMGenerator(n_data_col, n_hidden_layer_gen)\n",
    "    sam.reset_parameters()\n",
    "    sampler_graph = MatrixSampler(n_data_col, mask=None, gumbel=False)\n",
    "    sampler_neuron = MatrixSampler((n_hidden_layer_gen, n_data_col), mask=False, gumbel=True)\n",
    "\n",
    "    sampler_graph.weights.data.fill_(2)\n",
    "\n",
    "    discriminator = SAMDiscriminator(\n",
    "        n_data_col=n_data_col, n_hidden_layer=n_hidden_layer_dis, n_hidden_layers=n_hidden_layers_dis\n",
    "    )\n",
    "    discriminator.reset_parameters()  \n",
    "\n",
    "    optimizer_gen = optim.Adam(sam.parameters(), lr=lr_gen)\n",
    "    optimizer_dis = optim.Adam(discriminator.parameters(), lr=lr_dis)\n",
    "    optimizer_graph = optim.Adam(sampler_graph.parameters(), lr=lr_gen)\n",
    "    optimizer_neuron = optim.Adam(sampler_neuron.parameters(), lr=lr_gen)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    _true = torch.ones(1)\n",
    "    _false = torch.zeros(1)\n",
    "\n",
    "    noise = torch.randn(batch_size, n_data_col)\n",
    "    noise_row = torch.ones(1, n_data_col)\n",
    "\n",
    "    output = torch.zeros(n_data_col, n_data_col)\n",
    "    output_loss = torch.zeros(1, 1)\n",
    "\n",
    "    pbar = tqdm(range(epochs_train + epochs_test))\n",
    "    for epoch in pbar:\n",
    "        for loop_num, data_batched in enumerate(data_iterator):\n",
    "\n",
    "            optimizer_gen.zero_grad()\n",
    "            optimizer_graph.zero_grad()\n",
    "            optimizer_neuron.zero_grad()\n",
    "            optimizer_dis.zero_grad()\n",
    "\n",
    "            drawn_graph = sampler_graph()\n",
    "            drawn_neurons = sampler_neuron()\n",
    "\n",
    "            noise.normal_()\n",
    "            generated_variables = sam(\n",
    "                data=data_batched, \n",
    "                noise=noise,\n",
    "                adj_matrix=torch.cat(\n",
    "                    [drawn_graph, noise_row], 0\n",
    "                ), \n",
    "                drawn_neurons=drawn_neurons\n",
    "            )\n",
    "\n",
    "            dis_vars_d = discriminator(generated_variables.detach(), data_batched)\n",
    "            dis_vars_g = discriminator(generated_variables, data_batched)\n",
    "            true_vars_dis = discriminator(data_batched) \n",
    "\n",
    "            loss_dis = sum(\n",
    "                [criterion(gen, _false.expand_as(gen)) for gen in dis_vars_d]\n",
    "            ) / n_data_col + criterion(\n",
    "                true_vars_dis, _true.expand_as(true_vars_dis)\n",
    "            )\n",
    "\n",
    "            loss_gen = sum([criterion(gen, _true.expand_as(gen)) for gen in dis_vars_g])\n",
    "\n",
    "            if epoch < epochs_train:\n",
    "                loss_dis.backward()\n",
    "                optimizer_dis.step()\n",
    "\n",
    "            loss_struc = lambda1 / batch_size * drawn_graph.sum()     \n",
    "            loss_func = lambda2 / batch_size * drawn_neurons.sum()  \n",
    "\n",
    "            loss_regul = loss_struc + loss_func\n",
    "\n",
    "            if epoch <= epochs_train * dag_start_rate:\n",
    "                loss = loss_gen + loss_regul\n",
    "            else:\n",
    "                filters = sampler_graph.get_proba()\n",
    "                loss = loss_gen + loss_regul + (\n",
    "                    (epoch - epochs_train * dag_start_rate) * dag_penalization_increase\n",
    "                ) * notears_constr(filters * filters)\n",
    "\n",
    "            if epoch >= epochs_train:\n",
    "                output.add_(filters.data)\n",
    "                output_loss.add_(loss_gen.data)\n",
    "            else:\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer_gen.step()\n",
    "                optimizer_graph.step()\n",
    "                optimizer_neuron.step()\n",
    "\n",
    "            # 進捗の表示\n",
    "            if epoch % 50 == 0:\n",
    "                pbar.set_postfix(\n",
    "                    gen=loss_gen.item()/n_data_col,\n",
    "                    dis=loss_dis.item(),\n",
    "                    egul_loss=loss_regul.item(),\n",
    "                    tot=loss.item()\n",
    "                )\n",
    "\n",
    "    return output.cpu().numpy()/epochs_test, output_loss.cpu().numpy()/epochs_test/n_data_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data(n_data=2000)\n",
    "learning_data = data.copy()\n",
    "learning_data.loc[:, :] = scale(learning_data.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =learning_data.copy()\n",
    "n_hidden_layer_gen=100\n",
    "n_hidden_layer_dis=100\n",
    "n_hidden_layers_dis=2\n",
    "lr_gen=0.01*0.5\n",
    "lr_dis=0.01*0.5*2\n",
    "dag_start_rate=0.5\n",
    "dag_penalization_increase=0.001*10\n",
    "epochs_train=1000\n",
    "epochs_test=1000\n",
    "lambda1=5.0*20\n",
    "lambda2=0.005*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "data_columns = data.columns.tolist() \n",
    "n_data_col = len(data_columns)  \n",
    "data = torch.from_numpy(data.values.astype('float32') )\n",
    "batch_size = len(data)\n",
    "\n",
    "data_iterator = DataLoader(\n",
    "    data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "sam = SAMGenerator(n_data_col, n_hidden_layer_gen)\n",
    "sam.reset_parameters()\n",
    "sampler_graph = MatrixSampler(n_data_col, mask=None, gumbel=False)\n",
    "sampler_neuron = MatrixSampler((n_hidden_layer_gen, n_data_col), mask=False, gumbel=True)\n",
    "\n",
    "sampler_graph.weights.data.fill_(2)\n",
    "\n",
    "discriminator = SAMDiscriminator(\n",
    "    n_data_col=n_data_col, n_hidden_layer=n_hidden_layer_dis, n_hidden_layers=n_hidden_layers_dis\n",
    ")\n",
    "discriminator.reset_parameters()  \n",
    "\n",
    "optimizer_gen = optim.Adam(sam.parameters(), lr=lr_gen)\n",
    "optimizer_dis = optim.Adam(discriminator.parameters(), lr=lr_dis)\n",
    "optimizer_graph = optim.Adam(sampler_graph.parameters(), lr=lr_gen)\n",
    "optimizer_neuron = optim.Adam(sampler_neuron.parameters(), lr=lr_gen)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "_true = torch.ones(1)\n",
    "_false = torch.zeros(1)\n",
    "\n",
    "noise = torch.randn(batch_size, n_data_col)\n",
    "noise_row = torch.ones(1, n_data_col)\n",
    "\n",
    "output = torch.zeros(n_data_col, n_data_col)\n",
    "output_loss = torch.zeros(1, 1)\n",
    "\n",
    "pbar = tqdm(range(epochs_train + epochs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batched = list(data_iterator)[0]\n",
    "n_hidden_layer = n_hidden_layer_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6264,  0.9588, -0.4542,  1.4548,  1.5592,  1.0264],\n",
       "        [-1.5630,  0.9588, -0.2876,  0.6510,  0.1707,  0.0334],\n",
       "        [ 1.1218, -1.0429, -0.6249, -0.9565, -1.1501, -1.0986],\n",
       "        ...,\n",
       "        [ 1.2443, -1.0429, -0.0804, -0.1527, -0.0903,  0.0187],\n",
       "        [-1.3658,  0.9588,  0.0247,  1.4548,  1.4087,  0.5823],\n",
       "        [-1.5699,  0.9588, -0.4240,  1.4548,  0.9753,  0.9582]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen.zero_grad()\n",
    "optimizer_graph.zero_grad()\n",
    "optimizer_neuron.zero_grad()\n",
    "optimizer_dis.zero_grad()\n",
    "\n",
    "drawn_graph = sampler_graph()\n",
    "drawn_neurons = sampler_neuron()\n",
    "\n",
    "noise.normal_()\n",
    "generated_variables = sam(\n",
    "    data=data_batched, \n",
    "    noise=noise,\n",
    "    adj_matrix=torch.cat(\n",
    "        [drawn_graph, noise_row], 0\n",
    "    ), \n",
    "    drawn_neurons=drawn_neurons\n",
    ")\n",
    "\n",
    "dis_vars_d = discriminator(generated_variables.detach(), data_batched)\n",
    "dis_vars_g = discriminator(generated_variables, data_batched)\n",
    "true_vars_dis = discriminator(data_batched) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1235],\n",
       "         [ 0.1333],\n",
       "         [ 0.7230],\n",
       "         ...,\n",
       "         [ 0.4232],\n",
       "         [ 0.0601],\n",
       "         [ 0.1103]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.3398],\n",
       "         [ 0.3073],\n",
       "         [ 0.3554],\n",
       "         ...,\n",
       "         [-0.0626],\n",
       "         [ 0.3504],\n",
       "         [ 0.4491]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.5451],\n",
       "         [ 0.3470],\n",
       "         [ 0.5242],\n",
       "         ...,\n",
       "         [-0.0019],\n",
       "         [ 0.5945],\n",
       "         [ 0.7447]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.0488],\n",
       "         [ 0.1127],\n",
       "         [ 0.5327],\n",
       "         ...,\n",
       "         [-0.0641],\n",
       "         [ 0.0772],\n",
       "         [ 0.2441]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.6313],\n",
       "         [ 0.1817],\n",
       "         [ 0.1072],\n",
       "         ...,\n",
       "         [-0.2520],\n",
       "         [ 0.4559],\n",
       "         [ 0.6192]], grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.2091],\n",
       "         [ 0.2019],\n",
       "         [ 0.2617],\n",
       "         ...,\n",
       "         [-0.1229],\n",
       "         [ 0.2435],\n",
       "         [ 0.3586]], grad_fn=<AddmmBackward>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_vars_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen.zero_grad()\n",
    "optimizer_graph.zero_grad()\n",
    "optimizer_neuron.zero_grad()\n",
    "optimizer_dis.zero_grad()\n",
    "\n",
    "drawn_graph = sampler_graph()\n",
    "drawn_neurons = sampler_neuron()\n",
    "\n",
    "noise.normal_()\n",
    "adj_matrix = torch.cat([drawn_graph, noise_row], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Linear3D((n_data_col, n_data_col + 1, n_hidden_layer))\n",
    "layers = []\n",
    "layers.append(ChannelBatchNorm1d(n_data_col, n_hidden_layer))\n",
    "layers.append(nn.Tanh())\n",
    "layers = nn.Sequential(*layers)\n",
    "output_layer = Linear3D((n_data_col, n_hidden_layer, 1))\n",
    "skeleton = 1 - torch.eye(n_data_col + 1, n_data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix * skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = input_layer(data, noise, adj_matrix * skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output_layer(x, noise=None, adj_matrix=drawn_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2622e-01, -5.5772e-02, -7.1864e-02,  1.8798e-01,  2.8039e-01,\n",
       "         -8.9512e-02],\n",
       "        [ 2.4660e-01,  1.3123e-02, -2.2076e-01, -3.3251e-01,  4.4170e-01,\n",
       "         -2.0561e-01],\n",
       "        [-3.2613e-01,  1.2684e-01,  6.1894e-01,  4.3970e-01, -4.9755e-01,\n",
       "         -1.3570e-02],\n",
       "        ...,\n",
       "        [ 3.7394e-03,  1.9085e-01,  2.0638e-01, -2.7084e-01, -4.4423e-01,\n",
       "         -5.1361e-02],\n",
       "        [ 6.6384e-03,  7.7289e-02, -9.3609e-05, -2.3796e-01, -3.1015e-01,\n",
       "         -9.7169e-02],\n",
       "        [ 5.3628e-02,  9.9456e-02,  2.5218e-01,  5.6730e-02, -4.1738e-01,\n",
       "         -1.1370e-01]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.nn.Parameter(torch.Tensor(n_data_col, n_data_col + 1, n_hidden_layer))\n",
    "bias = torch.nn.Parameter(torch.Tensor(n_data_col, n_hidden_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_tmp = torch.cat([\n",
    "    data.unsqueeze(1).expand([data.shape[0], n_data_col, n_data_col]),\n",
    "    noise.unsqueeze(2)\n",
    "], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526,  1.2960],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526, -1.3462],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526, -0.8286],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526,  0.8917],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526,  0.6658],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526,  0.4814]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 7])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix.t().unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1., 1., 1.],\n",
       "         [1., 0., 1., 1., 1., 1., 1.],\n",
       "         [1., 1., 0., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 0., 1., 1.],\n",
       "         [1., 1., 1., 1., 1., 0., 1.]]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix.t().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp = h_tmp * adj_matrix.t().unsqueeze(0)\n",
    "h_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.9588, -0.0717, -0.9565, -0.7468, -0.8526, -1.8773],\n",
       "        [-1.0288,  0.0000, -0.0717, -0.9565, -0.7468, -0.8526, -1.0362],\n",
       "        [-1.0288,  0.9588, -0.0000, -0.9565, -0.7468, -0.8526,  0.3221],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.0000, -0.7468, -0.8526,  0.0459],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.0000, -0.8526, -0.2099],\n",
       "        [-1.0288,  0.9588, -0.0717, -0.9565, -0.7468, -0.0000,  0.9651]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.9588,  1.5529, -0.1527,  0.3245,  0.2860, -0.3984],\n",
       "        [ 0.4281,  0.0000,  1.5529, -0.1527,  0.3245,  0.2860, -0.7596],\n",
       "        [ 0.4281,  0.9588,  0.0000, -0.1527,  0.3245,  0.2860, -0.5563],\n",
       "        [ 0.4281,  0.9588,  1.5529, -0.0000,  0.3245,  0.2860, -1.7631],\n",
       "        [ 0.4281,  0.9588,  1.5529, -0.1527,  0.0000,  0.2860,  0.0732],\n",
       "        [ 0.4281,  0.9588,  1.5529, -0.1527,  0.3245,  0.0000,  0.1426]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0288,  0.0000, -0.0717,  ..., -0.7468, -0.8526, -1.0362],\n",
       "        [ 0.4281,  0.0000,  1.5529,  ...,  0.3245,  0.2860, -0.7596],\n",
       "        [-0.1957, -0.0000, -0.3203,  ...,  0.2314,  0.5084,  0.2556],\n",
       "        ...,\n",
       "        [ 1.2523, -0.0000, -0.2571,  ..., -0.0278,  0.3842, -0.1061],\n",
       "        [ 1.5519, -0.0000, -0.1780,  ...,  0.1190,  0.2537,  0.3969],\n",
       "        [ 1.2993, -0.0000, -0.4257,  ...,  0.6279,  0.8868,  1.2072]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_tmp.transpose(0, 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = h_tmp.transpose(0, 1).matmul(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.7978,  0.0000,  ..., -4.9201,  0.0000, -3.1332],\n",
       "        [ 0.0000,  1.7978,  0.0000,  ..., -0.1387,  0.0000,  4.0563],\n",
       "        [ 0.0000, -1.9555,  0.0000,  ...,  3.5570,  0.0000,  0.7865],\n",
       "        ...,\n",
       "        [ 0.0000, -1.9555,  0.0000,  ..., -1.0816,  0.0000,  0.1863],\n",
       "        [ 0.0000, -1.9555,  0.0000,  ..., -2.0183,  0.0000,  0.3650],\n",
       "        [ 0.0000, -1.9555,  0.0000,  ..., -2.3395,  0.0000,  2.0418]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = h_tmp.transpose(0, 1).matmul(weight)\n",
    "if bias is not None:\n",
    "    output += bias.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 6, 100])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_variables = sam(\n",
    "    data=data_batched, \n",
    "    noise=noise,\n",
    "    adj_matrix=torch.cat(\n",
    "        [drawn_graph, noise_row], 0\n",
    "    ), \n",
    "    drawn_neurons=drawn_neurons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layer = n_hidden_layer_dis\n",
    "n_hidden_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(n_data_col, n_hidden_layer).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layers.append(nn.Linear(n_data_col, n_hidden_layer))\n",
    "layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "layers.append(nn.LeakyReLU(.2))\n",
    "\n",
    "for i in range(n_hidden_layers - 1):\n",
    "    layers.append(nn.Linear(n_hidden_layer, n_hidden_layer))\n",
    "    layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "    layers.append(nn.LeakyReLU(.2))\n",
    "\n",
    "layers.append(nn.Linear(n_hidden_layer, 1))\n",
    "layers = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.eye(n_data_col, n_data_col).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5002, -0.2319,  0.2539, -0.5178,  0.2953, -0.2955],\n",
       "        [-0.2911,  0.2191, -0.2386, -0.1601, -0.0403, -0.0851],\n",
       "        [ 0.2335,  0.3069, -0.0659,  0.2923, -0.0417, -0.1788],\n",
       "        ...,\n",
       "        [-0.0857,  0.1863, -0.3762,  0.3394, -0.3351,  0.5045],\n",
       "        [-0.1296, -0.3794,  0.1099, -0.1596,  0.1346,  0.1124],\n",
       "        [-0.4049, -0.1997,  0.2635, -0.2337,  0.0892, -0.0347]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = data_batched.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5586, -1.0429, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "        [ 0.6651, -1.0429, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "        [ 1.6284, -1.0429,  0.2841, -0.1527, -0.2247,  0.1411],\n",
       "        ...,\n",
       "        [-0.2378,  0.9588,  0.5697, -0.9565,  0.2053,  0.3430],\n",
       "        [-1.6105,  0.9588, -0.4935,  1.4548,  0.5564,  0.4725],\n",
       "        [ 0.3310, -1.0429, -1.1649,  0.6510, -0.7197, -0.9972]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000, -1.0429, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "         [-1.5586, -0.0000, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "         [-1.5586, -1.0429, -0.0000,  0.6510, -1.2175, -1.5709],\n",
       "         [-1.5586, -1.0429, -1.6498,  0.0000, -1.2175, -1.5709],\n",
       "         [-1.5586, -1.0429, -1.6498,  0.6510, -0.0000, -1.5709],\n",
       "         [-1.5586, -1.0429, -1.6498,  0.6510, -1.2175, -0.0000]],\n",
       "\n",
       "        [[ 0.0000, -1.0429, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "         [ 0.6651, -0.0000, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "         [ 0.6651, -1.0429, -0.0000, -0.9565, -1.0417, -0.9757],\n",
       "         [ 0.6651, -1.0429, -0.2457, -0.0000, -1.0417, -0.9757],\n",
       "         [ 0.6651, -1.0429, -0.2457, -0.9565, -0.0000, -0.9757],\n",
       "         [ 0.6651, -1.0429, -0.2457, -0.9565, -1.0417, -0.0000]],\n",
       "\n",
       "        [[ 0.0000, -1.0429,  0.2841, -0.1527, -0.2247,  0.1411],\n",
       "         [ 1.6284, -0.0000,  0.2841, -0.1527, -0.2247,  0.1411],\n",
       "         [ 1.6284, -1.0429,  0.0000, -0.1527, -0.2247,  0.1411],\n",
       "         [ 1.6284, -1.0429,  0.2841, -0.0000, -0.2247,  0.1411],\n",
       "         [ 1.6284, -1.0429,  0.2841, -0.1527, -0.0000,  0.1411],\n",
       "         [ 1.6284, -1.0429,  0.2841, -0.1527, -0.2247,  0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0000,  0.9588,  0.5697, -0.9565,  0.2053,  0.3430],\n",
       "         [-0.2378,  0.0000,  0.5697, -0.9565,  0.2053,  0.3430],\n",
       "         [-0.2378,  0.9588,  0.0000, -0.9565,  0.2053,  0.3430],\n",
       "         [-0.2378,  0.9588,  0.5697, -0.0000,  0.2053,  0.3430],\n",
       "         [-0.2378,  0.9588,  0.5697, -0.9565,  0.0000,  0.3430],\n",
       "         [-0.2378,  0.9588,  0.5697, -0.9565,  0.2053,  0.0000]],\n",
       "\n",
       "        [[-0.0000,  0.9588, -0.4935,  1.4548,  0.5564,  0.4725],\n",
       "         [-1.6105,  0.0000, -0.4935,  1.4548,  0.5564,  0.4725],\n",
       "         [-1.6105,  0.9588, -0.0000,  1.4548,  0.5564,  0.4725],\n",
       "         [-1.6105,  0.9588, -0.4935,  0.0000,  0.5564,  0.4725],\n",
       "         [-1.6105,  0.9588, -0.4935,  1.4548,  0.0000,  0.4725],\n",
       "         [-1.6105,  0.9588, -0.4935,  1.4548,  0.5564,  0.0000]],\n",
       "\n",
       "        [[ 0.0000, -1.0429, -1.1649,  0.6510, -0.7197, -0.9972],\n",
       "         [ 0.3310, -0.0000, -1.1649,  0.6510, -0.7197, -0.9972],\n",
       "         [ 0.3310, -1.0429, -0.0000,  0.6510, -0.7197, -0.9972],\n",
       "         [ 0.3310, -1.0429, -1.1649,  0.0000, -0.7197, -0.9972],\n",
       "         [ 0.3310, -1.0429, -1.1649,  0.6510, -0.0000, -0.9972],\n",
       "         [ 0.3310, -1.0429, -1.1649,  0.6510, -0.7197, -0.0000]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_data.unsqueeze(1) * (1 - mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5002, -0.2319,  0.2539, -0.5178,  0.2953, -0.2955]],\n",
       "\n",
       "        [[-0.2911,  0.2191, -0.2386, -0.1601, -0.0403, -0.0851]],\n",
       "\n",
       "        [[ 0.2335,  0.3069, -0.0659,  0.2923, -0.0417, -0.1788]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0857,  0.1863, -0.3762,  0.3394, -0.3351,  0.5045]],\n",
       "\n",
       "        [[-0.1296, -0.3794,  0.1099, -0.1596,  0.1346,  0.1124]],\n",
       "\n",
       "        [[-0.4049, -0.1997,  0.2635, -0.2337,  0.0892, -0.0347]]],\n",
       "       grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_variables.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5002, -1.0429, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "        [-1.5586, -0.2319, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "        [-1.5586, -1.0429,  0.2539,  0.6510, -1.2175, -1.5709],\n",
       "        [-1.5586, -1.0429, -1.6498, -0.5178, -1.2175, -1.5709],\n",
       "        [-1.5586, -1.0429, -1.6498,  0.6510,  0.2953, -1.5709],\n",
       "        [-1.5586, -1.0429, -1.6498,  0.6510, -1.2175, -0.2955]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(obs_data.unsqueeze(1) * (1 - mask) + generated_variables.unsqueeze(1) * mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2911, -1.0429, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "        [ 0.6651,  0.2191, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "        [ 0.6651, -1.0429, -0.2386, -0.9565, -1.0417, -0.9757],\n",
       "        [ 0.6651, -1.0429, -0.2457, -0.1601, -1.0417, -0.9757],\n",
       "        [ 0.6651, -1.0429, -0.2457, -0.9565, -0.0403, -0.9757],\n",
       "        [ 0.6651, -1.0429, -0.2457, -0.9565, -1.0417, -0.0851]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(obs_data.unsqueeze(1) * (1 - mask) + generated_variables.unsqueeze(1) * mask)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.unbind(\n",
    "    obs_data.unsqueeze(1) * (1 - mask) + generated_variables.unsqueeze(1) * mask,\n",
    "    1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5002, -1.0429, -1.6498,  0.6510, -1.2175, -1.5709],\n",
       "        [-0.2911, -1.0429, -0.2457, -0.9565, -1.0417, -0.9757],\n",
       "        [ 0.2335, -1.0429,  0.2841, -0.1527, -0.2247,  0.1411],\n",
       "        ...,\n",
       "        [-0.0857,  0.9588,  0.5697, -0.9565,  0.2053,  0.3430],\n",
       "        [-0.1296,  0.9588, -0.4935,  1.4548,  0.5564,  0.4725],\n",
       "        [-0.4049, -1.0429, -1.1649,  0.6510, -0.7197, -0.9972]],\n",
       "       grad_fn=<UnbindBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [layers(i) for i in torch.unbind(\n",
    "    obs_data.unsqueeze(1) * (1 - mask) + generated_variables.unsqueeze(1) * mask,\n",
    "    1\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 12]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3]]).dot(np.array([[1, 2], [1, 2], [1, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatrixSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_size = n_data_col\n",
    "mask = None\n",
    "gumbel = False\n",
    "\n",
    "graph_size = (graph_size, graph_size)\n",
    "weights = th.nn.Parameter(th.FloatTensor(*graph_size))\n",
    "weights.data.zero_()\n",
    "if mask is None:\n",
    "    mask = 1 - th.eye(*graph_size)\n",
    "\n",
    "ones_tensor = th.ones(graph_size)\n",
    "zeros_tensor = th.zeros(*graph_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_logistic(shape, out=None):\n",
    "    U = out.resize_(shape).uniform_() if out is not None else th.rand(shape)\n",
    "    return th.log(U) - th.log(1-U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.9600,  3.0563, -0.8097,  0.4350,  1.2195,  0.5548,  1.5671,  4.2093,\n",
       "        -0.4976, -0.8441])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample_logistic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid_sample(logits, tau=1):\n",
    "    dims = logits.dim()\n",
    "    logistic_noise = _sample_logistic(logits.size(), out=logits.data.new())\n",
    "    y = logits + logistic_noise\n",
    "    return th.sigmoid(y / tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列にノイズを加える関数<br>\n",
    "hardをTrueにすると、0.5より大きい値の場合は１に、0.5未満の場合は０になるようにしている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_sigmoid(logits, ones_tensor, zeros_tensor, tau=1, hard=False):\n",
    "\n",
    "    shape = logits.size()\n",
    "    y_soft = _sigmoid_sample(logits, tau=tau)\n",
    "    if hard:\n",
    "        y_hard = th.where(y_soft > 0.5, ones_tensor, zeros_tensor)\n",
    "        y = y_hard.detach() - y_soft.detach() + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1\n",
    "drawhard = True\n",
    "drawn_proba = gumbel_sigmoid(\n",
    "    2 * weights, ones_tensor, zeros_tensor, tau=tau, hard=drawhard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 1.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sample_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = None\n",
    "gumbel = True\n",
    "graph_size = (n_hidden_layer_gen, n_data_col)\n",
    "\n",
    "weights = th.nn.Parameter(th.FloatTensor(*graph_size))\n",
    "weights.data.zero_()\n",
    "if mask is None:\n",
    "    mask = 1 - th.eye(*graph_size)\n",
    "\n",
    "ones_tensor = th.ones(graph_size)\n",
    "zeros_tensor = th.zeros(*graph_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 2])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.stack([weights.view(-1), -weights.view(-1)], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_gumbel(shape, eps=1e-10, out=None):\n",
    "    \"\"\"\n",
    "    Implementation of pytorch.\n",
    "    (https://github.com/pytorch/pytorch/blob/e4eee7c2cf43f4edba7a14687ad59d3ed61d9833/torch/nn/functional.py)\n",
    "    Sample from Gumbel(0, 1)\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    U = out.resize_(shape).uniform_() if out is not None else th.rand(shape)\n",
    "    return - th.log(eps - th.log(U + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n",
    "    dims = logits.dim()\n",
    "    gumbel_noise = _sample_gumbel(logits.size(), eps=eps, out=logits.data.new())\n",
    "    y = logits + gumbel_noise\n",
    "    return th.softmax(y / tau, dims-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10):\n",
    "   \n",
    "    shape = logits.size()\n",
    "    assert len(shape) == 2\n",
    "    y_soft = _gumbel_softmax_sample(logits, tau=tau, eps=eps)\n",
    "    if hard:\n",
    "        _, k = y_soft.data.max(-1)\n",
    "        # this bit is based on\n",
    "        # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5\n",
    "        y_hard = logits.data.new(*shape).zero_().scatter_(-1, k.view(-1, 1), 1.0)\n",
    "        # this cool bit of code achieves two things:\n",
    "        # - makes the output value exactly one-hot (since we add then\n",
    "        #   subtract y_soft value)\n",
    "        # - makes the gradient equal to y_soft gradient (since we strip\n",
    "        #   all other gradients)\n",
    "        y = y_hard - y_soft.data + y_soft\n",
    "    else:\n",
    "        y = y_soft\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawn_proba = gumbel_softmax(\n",
    "    th.stack([weights.view(-1), -weights.view(-1)], 1),\n",
    "    tau=tau, hard=drawhard\n",
    ")[:, 0].view(graph_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 1., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [0., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 1., 0., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 1.],\n",
       "        [0., 1., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 1.],\n",
       "        [1., 0., 0., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0., 1., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 1., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 1., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 1.],\n",
       "        [1., 1., 0., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 1., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 0., 1., 0., 1.],\n",
       "        [0., 1., 1., 0., 0., 1.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 0., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 1., 0., 1.],\n",
       "        [1., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 1., 0., 0., 0., 1.],\n",
       "        [1., 1., 0., 1., 1., 0.],\n",
       "        [0., 1., 0., 1., 0., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_graph = MatrixSampler(n_data_col, mask=None, gumbel=False)\n",
    "filters = sampler_graph.get_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notears_constr(adj_m, max_pow=None):\n",
    "    \n",
    "    m_exp = [adj_m]\n",
    "    if max_pow is None:\n",
    "        max_pow = adj_m.shape[1]\n",
    "    while (m_exp[-1].sum() > 0 and len(m_exp) < max_pow):\n",
    "        m_exp.append(m_exp[-1] @ adj_m / len(m_exp))\n",
    "        \n",
    "    return sum([i.diag().sum() for idx, i in enumerate(m_exp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters * filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3814, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notears_constr(filters * filters, max_pow=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_m = filters * filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_exp = [adj_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pow = adj_m.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (m_exp[-1].sum() > 0 and len(m_exp) < max_pow):\n",
    "    m_exp.append(m_exp[-1] @ adj_m / len(m_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "        [0.0625, 0.0000, 0.0625, 0.0625, 0.0625, 0.0625],\n",
       "        [0.0625, 0.0625, 0.0000, 0.0625, 0.0625, 0.0625],\n",
       "        [0.0625, 0.0625, 0.0625, 0.0000, 0.0625, 0.0625],\n",
       "        [0.0625, 0.0625, 0.0625, 0.0625, 0.0000, 0.0625],\n",
       "        [0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_m * adj_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.0000, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.0000, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.0000, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.0000]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[0.3125, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.3125, 0.2500, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.3125, 0.2500, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.3125, 0.2500, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.3125, 0.2500],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.3125]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[0.1562, 0.1641, 0.1641, 0.1641, 0.1641, 0.1641],\n",
       "         [0.1641, 0.1562, 0.1641, 0.1641, 0.1641, 0.1641],\n",
       "         [0.1641, 0.1641, 0.1562, 0.1641, 0.1641, 0.1641],\n",
       "         [0.1641, 0.1641, 0.1641, 0.1562, 0.1641, 0.1641],\n",
       "         [0.1641, 0.1641, 0.1641, 0.1641, 0.1562, 0.1641],\n",
       "         [0.1641, 0.1641, 0.1641, 0.1641, 0.1641, 0.1562]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[0.0684, 0.0677, 0.0677, 0.0677, 0.0677, 0.0677],\n",
       "         [0.0677, 0.0684, 0.0677, 0.0677, 0.0677, 0.0677],\n",
       "         [0.0677, 0.0677, 0.0684, 0.0677, 0.0677, 0.0677],\n",
       "         [0.0677, 0.0677, 0.0677, 0.0684, 0.0677, 0.0677],\n",
       "         [0.0677, 0.0677, 0.0677, 0.0677, 0.0684, 0.0677],\n",
       "         [0.0677, 0.0677, 0.0677, 0.0677, 0.0677, 0.0684]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212],\n",
       "         [0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212],\n",
       "         [0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212],\n",
       "         [0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212],\n",
       "         [0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212],\n",
       "         [0.0212, 0.0212, 0.0212, 0.0212, 0.0212, 0.0212]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " tensor([[0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053],\n",
       "         [0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053],\n",
       "         [0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053],\n",
       "         [0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053],\n",
       "         [0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053],\n",
       "         [0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053]],\n",
       "        grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
