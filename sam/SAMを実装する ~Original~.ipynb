{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAMを実装する 〜Original〜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from cdt.utils.torch import ChannelBatchNorm1d, MatrixSampler, Linear3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "np.set_printoptions(precision=2, floatmode='fixed', suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_data=2000):\n",
    "\n",
    "    x = np.random.uniform(low=-1, high=1, size=n_data)  # -1から1の一様乱数\n",
    "\n",
    "    e_z = np.random.randn(n_data)  # ノイズの生成\n",
    "    z_prob = scipy.special.expit(-5.0 * x + 5 * e_z)\n",
    "    Z = np.array([])\n",
    "\n",
    "    for i in range(n_data):\n",
    "        Z_i = np.random.choice(2, size=1, p=[1-z_prob[i], z_prob[i]])[0]\n",
    "        Z = np.append(Z, Z_i)\n",
    "\n",
    "    t = np.zeros(n_data)\n",
    "    for i in range(n_data):\n",
    "        if x[i] < 0:\n",
    "            t[i] = 0.5\n",
    "        elif x[i] >= 0 and x[i] < 0.5:\n",
    "            t[i] = 0.7\n",
    "        elif x[i] >= 0.5:\n",
    "            t[i] = 1.0\n",
    "\n",
    "    e_y = np.random.randn(n_data)\n",
    "    Y = 2.0 + t*Z + 0.3*x + 0.1*e_y \n",
    "\n",
    "    Y2 = np.random.choice(\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "        n_data, p=[0.1, 0.2, 0.3, 0.2, 0.2]\n",
    "    )\n",
    "\n",
    "    e_y3 = np.random.randn(n_data)\n",
    "    Y3 = 3 * Y + Y2 + e_y3\n",
    "\n",
    "    e_y4 = np.random.randn(n_data)\n",
    "    Y4 = 3 * Y3 + 2 * e_y4 + 5\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'x': x,\n",
    "        'Z': Z,\n",
    "        'Y': Y,\n",
    "        'Y2': Y2,\n",
    "        'Y3': Y3,\n",
    "        'Y4': Y4\n",
    "    })\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_data_col, n_hidden_layer, n_hidden_layers):\n",
    "        \n",
    "        super(SAMDiscriminator, self).__init__()\n",
    "        \n",
    "        self.n_data_col = n_data_col\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(n_data_col, n_hidden_layer))\n",
    "        layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "        layers.append(nn.LeakyReLU(.2))\n",
    "        \n",
    "        for i in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(n_hidden_layer, n_hidden_layer))\n",
    "            layers.append(nn.BatchNorm1d(n_hidden_layer))\n",
    "            layers.append(nn.LeakyReLU(.2))\n",
    "            \n",
    "        layers.append(nn.Linear(n_hidden_layer, 1))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'mask', torch.eye(n_data_col, n_data_col).unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, obs_data=None):\n",
    "        \n",
    "        if obs_data is not None:\n",
    "            return [\n",
    "                self.layers(i) for i in torch.unbind(\n",
    "                    obs_data.unsqueeze(1) * (1 - self.mask) + input.unsqueeze(1) * self.mask,\n",
    "                    1\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            return self.layers(input)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_data_col, n_hidden_layer):\n",
    "        \n",
    "        super(SAMGenerator, self).__init__()\n",
    "        \n",
    "        skeleton = 1 - torch.eye(n_data_col + 1, n_data_col)\n",
    "        \n",
    "        self.register_buffer('skeleton', skeleton)\n",
    "        \n",
    "        self.input_layer = Linear3D((n_data_col, n_data_col + 1, n_hidden_layer))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(ChannelBatchNorm1d(n_data_col, n_hidden_layer))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.output_layer = Linear3D((n_data_col, n_hidden_layer, 1))\n",
    "        \n",
    "    def forward(self, data, noise, adj_matrix, drawn_neurons=None):\n",
    "        \n",
    "        x = self.input_layer(data, noise, adj_matrix * self.skeleton)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        output = self.output_layer(x, noise=None, adj_matrix=drawn_neurons)\n",
    "        \n",
    "        return output.squeeze(2)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        self.input_layer.reset_parameters()\n",
    "        self.output_layer.reset_parameters()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'reset_parametres'):\n",
    "                layer.register_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notears_constr(adj_m, max_pow=None):\n",
    "    \n",
    "    m_exp = [adj_m]\n",
    "    if max_pow is None:\n",
    "        max_pow = adj_m.shape[1]\n",
    "    while (m_exp[-1].sum() > 0 and len(m_exp) < max_pow):\n",
    "        m_exp.append(m_exp[-1] @ adj_m / len(m_exp))\n",
    "        \n",
    "    return sum([i.diag().sum() for idx, i in enumerate(m_exp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam_model(\n",
    "    data,\n",
    "    n_hidden_layer_gen=100, n_hidden_layer_dis=100,\n",
    "    n_hidden_layers_dis=2,\n",
    "    lr_gen=0.01*0.5, lr_dis=0.01*0.5*2,\n",
    "    dag_start_rate=0.5, dag_penalization_increase=0.001*10,\n",
    "    epochs_train=100, epochs_test=100,\n",
    "    lambda1=5.0*20, lambda2=0.005*20\n",
    "):\n",
    "\n",
    "    data_columns = data.columns.tolist() \n",
    "    n_data_col = len(data_columns)  \n",
    "    data = torch.from_numpy(data.values.astype('float32') )\n",
    "    batch_size = len(data)\n",
    "\n",
    "    data_iterator = DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    "    )\n",
    "\n",
    "    sam = SAMGenerator(n_data_col, n_hidden_layer_gen)\n",
    "    sam.reset_parameters()\n",
    "    sampler_graph = MatrixSampler(n_data_col, mask=None, gumbel=False)\n",
    "    sampler_neuron = MatrixSampler((n_hidden_layer_gen, n_data_col), mask=False, gumbel=True)\n",
    "\n",
    "    sampler_graph.weights.data.fill_(2)\n",
    "\n",
    "    discriminator = SAMDiscriminator(\n",
    "        n_data_col=n_data_col, n_hidden_layer=n_hidden_layer_dis, n_hidden_layers=n_hidden_layers_dis\n",
    "    )\n",
    "    discriminator.reset_parameters()  \n",
    "\n",
    "    optimizer_gen = optim.Adam(sam.parameters(), lr=lr_gen)\n",
    "    optimizer_dis = optim.Adam(discriminator.parameters(), lr=lr_dis)\n",
    "    optimizer_graph = optim.Adam(sampler_graph.parameters(), lr=lr_gen)\n",
    "    optimizer_neuron = optim.Adam(sampler_neuron.parameters(), lr=lr_gen)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    _true = torch.ones(1)\n",
    "    _false = torch.zeros(1)\n",
    "\n",
    "    noise = torch.randn(batch_size, n_data_col)\n",
    "    noise_row = torch.ones(1, n_data_col)\n",
    "\n",
    "    output = torch.zeros(n_data_col, n_data_col)\n",
    "    output_loss = torch.zeros(1, 1)\n",
    "\n",
    "    pbar = tqdm(range(epochs_train + epochs_test))\n",
    "    for epoch in pbar:\n",
    "        for loop_num, data_batched in enumerate(data_iterator):\n",
    "\n",
    "            optimizer_gen.zero_grad()\n",
    "            optimizer_graph.zero_grad()\n",
    "            optimizer_neuron.zero_grad()\n",
    "            optimizer_dis.zero_grad()\n",
    "\n",
    "            drawn_graph = sampler_graph()\n",
    "            drawn_neurons = sampler_neuron()\n",
    "\n",
    "            noise.normal_()\n",
    "            generated_variables = sam(\n",
    "                data=data_batched, \n",
    "                noise=noise,\n",
    "                adj_matrix=torch.cat(\n",
    "                    [drawn_graph, noise_row], 0\n",
    "                ), \n",
    "                drawn_neurons=drawn_neurons\n",
    "            )\n",
    "\n",
    "            dis_vars_d = discriminator(generated_variables.detach(), data_batched)\n",
    "            dis_vars_g = discriminator(generated_variables, data_batched)\n",
    "            true_vars_dis = discriminator(data_batched) \n",
    "\n",
    "            loss_dis = sum(\n",
    "                [criterion(gen, _false.expand_as(gen)) for gen in dis_vars_d]\n",
    "            ) / n_data_col + criterion(\n",
    "                true_vars_dis, _true.expand_as(true_vars_dis)\n",
    "            )\n",
    "\n",
    "            loss_gen = sum([criterion(gen, _true.expand_as(gen)) for gen in dis_vars_g])\n",
    "\n",
    "            if epoch < epochs_train:\n",
    "                loss_dis.backward()\n",
    "                optimizer_dis.step()\n",
    "\n",
    "            loss_struc = lambda1 / batch_size * drawn_graph.sum()     \n",
    "            loss_func = lambda2 / batch_size * drawn_neurons.sum()  \n",
    "\n",
    "            loss_regul = loss_struc + loss_func\n",
    "\n",
    "            if epoch <= epochs_train * dag_start_rate:\n",
    "                loss = loss_gen + loss_regul\n",
    "            else:\n",
    "                filters = sampler_graph.get_proba()\n",
    "                loss = loss_gen + loss_regul + (\n",
    "                    (epoch - epochs_train * dag_start_rate) * dag_penalization_increase\n",
    "                ) * notears_constr(filters * filters)\n",
    "\n",
    "            if epoch >= epochs_train:\n",
    "                output.add_(filters.data)\n",
    "                output_loss.add_(loss_gen.data)\n",
    "            else:\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer_gen.step()\n",
    "                optimizer_graph.step()\n",
    "                optimizer_neuron.step()\n",
    "\n",
    "            # 進捗の表示\n",
    "            if epoch % 50 == 0:\n",
    "                pbar.set_postfix(\n",
    "                    gen=loss_gen.item()/n_data_col,\n",
    "                    dis=loss_dis.item(),\n",
    "                    egul_loss=loss_regul.item(),\n",
    "                    tot=loss.item()\n",
    "                )\n",
    "\n",
    "    return output.cpu().numpy()/epochs_test, output_loss.cpu().numpy()/epochs_test/n_data_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_data(n_data=2000)\n",
    "learning_data = data.copy()\n",
    "learning_data.loc[:, :] = scale(learning_data.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:38<00:00,  3.86it/s, dis=0.681, egul_loss=0.559, gen=3.26, tot=32.5]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.95]]\n",
      "[[0.00 0.20 0.15 0.03 0.05 0.04]\n",
      " [0.91 0.00 0.53 0.42 0.11 0.47]\n",
      " [0.41 0.62 0.00 0.83 0.30 0.58]\n",
      " [0.05 0.02 0.02 0.00 0.06 0.02]\n",
      " [0.14 0.05 0.25 0.94 0.00 0.52]\n",
      " [0.07 0.05 0.19 0.39 0.90 0.00]]\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:14<00:00,  3.25it/s, dis=1.11, egul_loss=0.561, gen=5.09, tot=46.6] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.40]]\n",
      "[[0.00 0.05 0.63 0.05 0.04 0.05]\n",
      " [0.89 0.00 0.65 0.52 0.17 0.56]\n",
      " [0.04 0.49 0.00 0.32 0.25 0.77]\n",
      " [0.04 0.03 0.03 0.00 0.07 0.07]\n",
      " [0.64 0.01 0.07 0.89 0.00 0.63]\n",
      " [0.40 0.07 0.04 0.83 0.84 0.00]]\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:44<00:00,  3.81it/s, dis=0.896, egul_loss=0.408, gen=2.58, tot=21.9]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.98]]\n",
      "[[0.00 0.20 0.42 0.04 0.03 0.08]\n",
      " [0.54 0.00 0.22 0.49 0.12 0.17]\n",
      " [0.15 0.85 0.00 0.85 0.73 0.18]\n",
      " [0.07 0.02 0.02 0.00 0.17 0.02]\n",
      " [0.23 0.03 0.05 0.89 0.00 0.90]\n",
      " [0.14 0.07 0.07 0.45 0.33 0.00]]\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:02<00:00,  3.32it/s, dis=0.907, egul_loss=0.56, gen=1.84, tot=26.5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.64]]\n",
      "[[0.00 0.85 0.63 0.18 0.05 0.05]\n",
      " [0.09 0.00 0.39 0.58 0.09 0.02]\n",
      " [0.08 0.93 0.00 0.81 0.48 0.16]\n",
      " [0.03 0.02 0.04 0.00 0.13 0.03]\n",
      " [0.11 0.03 0.21 0.87 0.00 0.82]\n",
      " [0.12 0.04 0.17 0.72 0.69 0.00]]\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:51<00:00,  3.38it/s, dis=0.512, egul_loss=0.458, gen=4.67, tot=41.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.17]]\n",
      "[[0.00 0.25 0.12 0.18 0.03 0.05]\n",
      " [0.89 0.00 0.51 0.10 0.12 0.07]\n",
      " [0.29 0.93 0.00 0.86 0.56 0.03]\n",
      " [0.04 0.03 0.05 0.00 0.10 0.03]\n",
      " [0.38 0.02 0.14 0.88 0.00 0.90]\n",
      " [0.22 0.16 0.11 0.59 0.40 0.00]]\n",
      "[[0.00 0.31 0.39 0.09 0.04 0.06]\n",
      " [0.66 0.00 0.46 0.42 0.12 0.26]\n",
      " [0.19 0.76 0.00 0.73 0.47 0.35]\n",
      " [0.05 0.02 0.03 0.00 0.11 0.03]\n",
      " [0.30 0.03 0.14 0.89 0.00 0.75]\n",
      " [0.19 0.08 0.12 0.59 0.63 0.00]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "m_list = []\n",
    "loss_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    m, loss = train_sam_model(\n",
    "        data=learning_data.copy(),\n",
    "        n_hidden_layer_gen=100, n_hidden_layer_dis=100,\n",
    "        n_hidden_layers_dis=2,\n",
    "        lr_gen=0.01*0.5, lr_dis=0.01*0.5*2,\n",
    "        dag_start_rate=0.5,\n",
    "        dag_penalization_increase=0.001*10,\n",
    "        epochs_train=1000, epochs_test=1000,\n",
    "        lambda1=5.0 * 20, lambda2=0.005 * 20\n",
    "    )\n",
    "\n",
    "    print(loss)\n",
    "    print(m)\n",
    "\n",
    "    m_list.append(m)\n",
    "    loss_list.append(loss)\n",
    "\n",
    "# ネットワーク構造（5回の平均）\n",
    "print(sum(m_list) / len(m_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=learning_data.copy()\n",
    "n_hidden_layer_gen=200\n",
    "n_hidden_layer_dis=200\n",
    "n_hidden_layers_dis=2\n",
    "lr_gen=0.01*0.5\n",
    "lr_dis=0.01*0.5*2\n",
    "dag_start_rate=0.5\n",
    "dag_penalization_increase=0.001*10\n",
    "epochs_train=10000\n",
    "epochs_test=1000\n",
    "lambda1=5.0*20\n",
    "lambda2=0.005*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = data.columns.tolist() \n",
    "n_data_col = len(data_columns)  \n",
    "data = torch.from_numpy(data.values.astype('float32') )\n",
    "batch_size = len(data)\n",
    "\n",
    "data_iterator = DataLoader(\n",
    "    data, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "sam = SAMGenerator(n_data_col, n_hidden_layer_gen)\n",
    "sam.reset_parameters()\n",
    "sampler_graph = MatrixSampler(n_data_col, mask=None, gumbel=False)\n",
    "sampler_neuron = MatrixSampler((n_hidden_layer_gen, n_data_col), mask=False, gumbel=True)\n",
    "\n",
    "sampler_graph.weights.data.fill_(2)\n",
    "\n",
    "discriminator = SAMDiscriminator(\n",
    "    n_data_col=n_data_col, n_hidden_layer=n_hidden_layer_dis, n_hidden_layers=n_hidden_layers_dis\n",
    ")\n",
    "discriminator.reset_parameters()  \n",
    "\n",
    "optimizer_gen = optim.Adam(sam.parameters(), lr=lr_gen)\n",
    "optimizer_dis = optim.Adam(discriminator.parameters(), lr=lr_dis)\n",
    "optimizer_graph = optim.Adam(sampler_graph.parameters(), lr=lr_gen)\n",
    "optimizer_neuron = optim.Adam(sampler_neuron.parameters(), lr=lr_gen)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "_true = torch.ones(1)\n",
    "_false = torch.zeros(1)\n",
    "\n",
    "noise = torch.randn(batch_size, n_data_col)\n",
    "noise_row = torch.ones(1, n_data_col)\n",
    "\n",
    "output = torch.zeros(n_data_col, n_data_col)\n",
    "output_loss = torch.zeros(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 133/11000 [01:34<2:08:22,  1.41it/s, dis=1.39, egul_loss=1.42, gen=0.694, tot=5.59]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-caa8dc614307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0moutput_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs_train + epochs_test))\n",
    "for epoch in pbar:\n",
    "    for loop_num, data_batched in enumerate(data_iterator):\n",
    "\n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_graph.zero_grad()\n",
    "        optimizer_neuron.zero_grad()\n",
    "        optimizer_dis.zero_grad()\n",
    "\n",
    "        drawn_graph = sampler_graph()\n",
    "        drawn_neurons = sampler_neuron()\n",
    "\n",
    "        noise.normal_()\n",
    "        generated_variables = sam(\n",
    "            data=data_batched, \n",
    "            noise=noise,\n",
    "            adj_matrix=torch.cat(\n",
    "                [drawn_graph, noise_row], 0\n",
    "            ), drawn_neurons=drawn_neurons\n",
    "        )\n",
    "\n",
    "        dis_vars_d = discriminator(generated_variables.detach(), data_batched)\n",
    "        dis_vars_g = discriminator(generated_variables, data_batched)\n",
    "        true_vars_dis = discriminator(data_batched) \n",
    "\n",
    "        loss_dis = sum(\n",
    "            [criterion(gen, _false.expand_as(gen)) for gen in dis_vars_d]\n",
    "        ) / n_data_col + criterion(\n",
    "            true_vars_dis, _true.expand_as(true_vars_dis)\n",
    "        )\n",
    "\n",
    "        loss_gen = sum([criterion(gen, _true.expand_as(gen)) for gen in dis_vars_g])\n",
    "\n",
    "        if epoch < epochs_train:\n",
    "            loss_dis.backward()\n",
    "            optimizer_dis.step()\n",
    "\n",
    "        loss_struc = lambda1 / batch_size * drawn_graph.sum()     \n",
    "        loss_func = lambda2 / batch_size * drawn_neurons.sum()  \n",
    "\n",
    "        loss_regul = loss_struc + loss_func\n",
    "\n",
    "        if epoch <= epochs_train * dag_start_rate:\n",
    "            loss = loss_gen + loss_regul\n",
    "        else:\n",
    "            filters = sampler_graph.get_proba()\n",
    "            loss = loss_gen + loss_regul + (\n",
    "                (epoch - epochs_train * dag_start_rate) * dag_penalization_increase\n",
    "            ) * notears_constr(filters * filters)\n",
    "\n",
    "        if epoch >= epochs_train:\n",
    "            output.add_(filters.data)\n",
    "            output_loss.add_(loss_gen.data)\n",
    "        else:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_gen.step()\n",
    "            optimizer_graph.step()\n",
    "            optimizer_neuron.step()\n",
    "\n",
    "        # 進捗の表示\n",
    "        if epoch % 50 == 0:\n",
    "            pbar.set_postfix(\n",
    "                gen=loss_gen.item()/n_data_col,\n",
    "                dis=loss_dis.item(),\n",
    "                egul_loss=loss_regul.item(),\n",
    "                tot=loss.item()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in pbar:\n",
    "    for i_batch, data_batched in enumerate(data_iterator):\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
